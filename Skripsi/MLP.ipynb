{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-64156d691fe5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../input/train.csv')\n",
    "test_data= pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1b396fc5-c81e-4be9-b6a7-0608b7cb7eac",
    "_uuid": "701b202ed145df665299de1a9a9872211a3d4a8d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert categorical values into numeric \n",
    "cleanup = { 'Sex': {'male':1, 'female':0}, 'Embarked':{'S':0, 'C':1, 'Q':2}}\n",
    "train_data.replace(cleanup, inplace= True)\n",
    "test_data.replace(cleanup, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "38b33816-243c-42e5-bffb-25ef510d5545",
    "_uuid": "c4f8025ccd3510d63e2d7d7a5ed01b22453e8b25",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Keep only relevant features (name and passenger ID are not relevant)\n",
    "train_data= train_data[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Embarked']]\n",
    "test_data= test_data[[ 'Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Embarked']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eebb26b0-6758-4b02-b445-b2768cd667f3",
    "_uuid": "376bf712b38aa312bf5fdaba1352f7c9325a10ff",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop null and missing values\n",
    "train_data = train_data.dropna()\n",
    "test_data = test_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a7e13762-4a22-4df8-9f85-4075fc5875c3",
    "_uuid": "e31a1ac36086cbfb036f354e53cf02ccbd355f13",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Scale all attribute values to between 0 and 1\n",
    "min_max=MinMaxScaler()\n",
    "Train_data_scaled =min_max.fit_transform(train_data[['Survived','Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Embarked']])\n",
    "Test_data_scaled =min_max.fit_transform(test_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Embarked']])\n",
    "print(Train_data_scaled.shape) #Number of rows in training set\n",
    "\n",
    "print(Test_data_scaled.shape) #number of rows in test set \n",
    "\n",
    "Xtrain = []\n",
    "Ytrain = []\n",
    "for i in range(Train_data_scaled.shape[0]):\n",
    "    Ytrain.append(Train_data_scaled[i][0])\n",
    "    Xtrain.append(list(Train_data_scaled[i][1:]))\n",
    "Xtrain = np.array(Xtrain)\n",
    "Ytrain = np.array(Ytrain)\n",
    "Train_data_Xtf = tf.convert_to_tensor(Xtrain, np.float32)\n",
    "Train_data_Ytf = tf.convert_to_tensor(Ytrain, np.float32)\n",
    "\n",
    "Test_data_Xtf = tf.convert_to_tensor(Test_data_scaled, np.float32)\n",
    "Test_data_Xtf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9658172d-c7d4-4601-a37b-a1f0bf99b32c",
    "_uuid": "781fefedac270db92736e78e821394ac2fcc5a3c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15 #Number of iterations\n",
    "batch_size = 50 #50 samples per iteration\n",
    "display_step =1\n",
    "hidden1 =16  #16 neurons in first hidden layer\n",
    "hidden2 = 16 \n",
    "inputlayer = 6 #6 inputs - Pclass, Sex, Age, sibsp, Fare, Embarked\n",
    "outputlayer = 1 #1 output - survived \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "db2d7127-4040-4b91-bf3c-34b5727ae469",
    "_uuid": "b88a849a9e3ac3e808521078198ee448be176d32",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Placeholders for X - input and Y - output\n",
    "X = tf.placeholder(\"float\", [None, inputlayer])\n",
    "Y = tf.placeholder(\"float\", [None, outputlayer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1a5ef6cb-42fc-4ca9-aa82-eb8c9f7375c0",
    "_uuid": "85da88c6a310a2afd91ded73f525fe3257b19cc8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Randomly initialized weights and biases\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([inputlayer, hidden1])),\n",
    "    'h2': tf.Variable(tf.random_normal([hidden1, hidden2])),\n",
    "    'out': tf.Variable(tf.random_normal([hidden2, outputlayer]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([hidden1])),\n",
    "    'b2': tf.Variable(tf.random_normal([hidden2])),\n",
    "    'out': tf.Variable(tf.random_normal([outputlayer]))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "96b1da39-d077-43a6-b82e-ce022ae85b54",
    "_uuid": "3c34eb6b4523efa7119164109d87a210cac10ae1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    logits=multilayer_perceptron(X), labels=Y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(int(Train_data_Xtf.get_shape()[0])/batch_size)\n",
    "        # Loop over all batches\n",
    "        print(total_batch)\n",
    "        for i in range(total_batch):\n",
    "            batch_x = Train_data_Xtf[i*total_batch: total_batch+i*total_batch+1].eval()\n",
    "            batch_y = Train_data_Ytf[i*total_batch: total_batch+i*total_batch+1].eval()\n",
    "            batch_x=batch_x.reshape(-1,6)\n",
    "            batch_y=batch_y.reshape(-1,1)\n",
    "            \n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([train_op, loss_op], feed_dict={X: batch_x,\n",
    "                                                            Y: batch_y})\n",
    "            \n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost={:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    pred = tf.nn.softmax(multilayer_perceptron(X))  # Apply softmax to logits\n",
    "    predicted_class = tf.greater(pred,0.5)\n",
    "    correct = tf.equal(predicted_class, tf.equal(Y,1.0))\n",
    "    accuracy = tf.reduce_mean( tf.cast(correct, 'float') )\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: Test_data_Xtf.eval().reshape(-1,6), Y: Train_data_Ytf[:331].eval().reshape(-1,1)})\n",
    "    print (\"Test accuracy: %.3f\" % (test_acc))\n",
    "                        \n",
    "                                      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "57c4daa8-0ec7-4be3-a607-7065012584c3",
    "_uuid": "496f1e8b69fb4428bb7cf8c0970312a4500bce22",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ee26600a-3591-4510-bc5c-262df2652401",
    "_uuid": "bb5e753f7a8e026a1725ef00a65b1a5453f24bd6",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
