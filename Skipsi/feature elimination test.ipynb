{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset = pd.read_excel('feature elimination.xlsx', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Jenis</th>\n",
       "      <th>Solidity_Inti</th>\n",
       "      <th>Keliling_Inti</th>\n",
       "      <th>KelilingNormal_I</th>\n",
       "      <th>Granularity_Inti</th>\n",
       "      <th>RerataGP</th>\n",
       "      <th>GranularityP</th>\n",
       "      <th>RerataBP</th>\n",
       "      <th>KontrasP</th>\n",
       "      <th>Solidity_Inti.1</th>\n",
       "      <th>...</th>\n",
       "      <th>RerataB_Inti</th>\n",
       "      <th>LuasNormal_P</th>\n",
       "      <th>EnergiP</th>\n",
       "      <th>Entropi_Inti</th>\n",
       "      <th>Energi_Inti</th>\n",
       "      <th>LuasP</th>\n",
       "      <th>Luas_Inti</th>\n",
       "      <th>KIperKP</th>\n",
       "      <th>RerataG_Inti</th>\n",
       "      <th>Kontras_Inti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.295574</td>\n",
       "      <td>2235</td>\n",
       "      <td>2.337866</td>\n",
       "      <td>104.162460</td>\n",
       "      <td>13.242218</td>\n",
       "      <td>67.830696</td>\n",
       "      <td>20.572479</td>\n",
       "      <td>411.989685</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>61.313919</td>\n",
       "      <td>0.391224</td>\n",
       "      <td>0.785876</td>\n",
       "      <td>1.517679</td>\n",
       "      <td>0.363583</td>\n",
       "      <td>28320</td>\n",
       "      <td>22333</td>\n",
       "      <td>1.041958</td>\n",
       "      <td>8.136516</td>\n",
       "      <td>125.540268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.276264</td>\n",
       "      <td>4125</td>\n",
       "      <td>3.669929</td>\n",
       "      <td>63.401608</td>\n",
       "      <td>22.188545</td>\n",
       "      <td>71.498077</td>\n",
       "      <td>32.688419</td>\n",
       "      <td>423.807465</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>26.692270</td>\n",
       "      <td>0.393040</td>\n",
       "      <td>0.577366</td>\n",
       "      <td>1.428150</td>\n",
       "      <td>0.359131</td>\n",
       "      <td>48863</td>\n",
       "      <td>31025</td>\n",
       "      <td>0.867508</td>\n",
       "      <td>9.076974</td>\n",
       "      <td>71.531929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.274679</td>\n",
       "      <td>7965</td>\n",
       "      <td>6.926087</td>\n",
       "      <td>65.156616</td>\n",
       "      <td>18.550638</td>\n",
       "      <td>69.066055</td>\n",
       "      <td>28.637329</td>\n",
       "      <td>373.516144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>31.138128</td>\n",
       "      <td>0.446200</td>\n",
       "      <td>0.599164</td>\n",
       "      <td>1.648434</td>\n",
       "      <td>0.293116</td>\n",
       "      <td>54135</td>\n",
       "      <td>36832</td>\n",
       "      <td>2.514205</td>\n",
       "      <td>11.777470</td>\n",
       "      <td>127.077431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.263279</td>\n",
       "      <td>8115</td>\n",
       "      <td>7.431319</td>\n",
       "      <td>66.084785</td>\n",
       "      <td>18.314508</td>\n",
       "      <td>66.335373</td>\n",
       "      <td>28.886127</td>\n",
       "      <td>411.183746</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>34.198906</td>\n",
       "      <td>0.469368</td>\n",
       "      <td>0.577987</td>\n",
       "      <td>1.662973</td>\n",
       "      <td>0.267211</td>\n",
       "      <td>51431</td>\n",
       "      <td>34974</td>\n",
       "      <td>2.064885</td>\n",
       "      <td>13.302699</td>\n",
       "      <td>126.954521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.228109</td>\n",
       "      <td>9930</td>\n",
       "      <td>8.316583</td>\n",
       "      <td>66.808174</td>\n",
       "      <td>25.652452</td>\n",
       "      <td>76.921638</td>\n",
       "      <td>39.452927</td>\n",
       "      <td>449.707581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>27.163689</td>\n",
       "      <td>0.357650</td>\n",
       "      <td>0.475496</td>\n",
       "      <td>1.343836</td>\n",
       "      <td>0.394241</td>\n",
       "      <td>57700</td>\n",
       "      <td>31828</td>\n",
       "      <td>2.439204</td>\n",
       "      <td>11.433691</td>\n",
       "      <td>147.499649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.242267</td>\n",
       "      <td>6978</td>\n",
       "      <td>6.533708</td>\n",
       "      <td>53.952385</td>\n",
       "      <td>22.601267</td>\n",
       "      <td>69.807327</td>\n",
       "      <td>34.568325</td>\n",
       "      <td>315.389404</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>19.595343</td>\n",
       "      <td>0.332632</td>\n",
       "      <td>0.473613</td>\n",
       "      <td>1.230935</td>\n",
       "      <td>0.428663</td>\n",
       "      <td>44690</td>\n",
       "      <td>23701</td>\n",
       "      <td>2.581576</td>\n",
       "      <td>8.621966</td>\n",
       "      <td>90.372208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.242138</td>\n",
       "      <td>7173</td>\n",
       "      <td>5.766077</td>\n",
       "      <td>63.488342</td>\n",
       "      <td>29.484987</td>\n",
       "      <td>79.742218</td>\n",
       "      <td>42.271564</td>\n",
       "      <td>481.051636</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>24.819304</td>\n",
       "      <td>0.330642</td>\n",
       "      <td>0.464251</td>\n",
       "      <td>1.261649</td>\n",
       "      <td>0.434828</td>\n",
       "      <td>61094</td>\n",
       "      <td>31947</td>\n",
       "      <td>1.166911</td>\n",
       "      <td>9.485422</td>\n",
       "      <td>99.338615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.273889</td>\n",
       "      <td>4785</td>\n",
       "      <td>4.139273</td>\n",
       "      <td>66.707657</td>\n",
       "      <td>26.333082</td>\n",
       "      <td>78.260277</td>\n",
       "      <td>36.513149</td>\n",
       "      <td>429.702301</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>25.246431</td>\n",
       "      <td>0.343561</td>\n",
       "      <td>0.523086</td>\n",
       "      <td>1.265952</td>\n",
       "      <td>0.420945</td>\n",
       "      <td>50622</td>\n",
       "      <td>28686</td>\n",
       "      <td>1.000627</td>\n",
       "      <td>10.557991</td>\n",
       "      <td>79.043915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0.268239</td>\n",
       "      <td>5421</td>\n",
       "      <td>5.346154</td>\n",
       "      <td>60.061481</td>\n",
       "      <td>21.753721</td>\n",
       "      <td>71.431885</td>\n",
       "      <td>33.718357</td>\n",
       "      <td>385.725739</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>26.141420</td>\n",
       "      <td>0.438259</td>\n",
       "      <td>0.527767</td>\n",
       "      <td>1.593338</td>\n",
       "      <td>0.302863</td>\n",
       "      <td>44634</td>\n",
       "      <td>28145</td>\n",
       "      <td>1.757782</td>\n",
       "      <td>9.860604</td>\n",
       "      <td>99.393921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0.272751</td>\n",
       "      <td>6276</td>\n",
       "      <td>5.664260</td>\n",
       "      <td>58.653553</td>\n",
       "      <td>25.858793</td>\n",
       "      <td>74.579300</td>\n",
       "      <td>39.881699</td>\n",
       "      <td>343.112488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>24.212234</td>\n",
       "      <td>0.348072</td>\n",
       "      <td>0.473379</td>\n",
       "      <td>1.288087</td>\n",
       "      <td>0.410599</td>\n",
       "      <td>49300</td>\n",
       "      <td>26639</td>\n",
       "      <td>1.874552</td>\n",
       "      <td>9.376661</td>\n",
       "      <td>98.397949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0.253547</td>\n",
       "      <td>12681</td>\n",
       "      <td>9.149351</td>\n",
       "      <td>68.033195</td>\n",
       "      <td>23.991095</td>\n",
       "      <td>73.897148</td>\n",
       "      <td>34.626331</td>\n",
       "      <td>525.832275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>32.101109</td>\n",
       "      <td>0.457309</td>\n",
       "      <td>0.522469</td>\n",
       "      <td>1.658149</td>\n",
       "      <td>0.280668</td>\n",
       "      <td>85727</td>\n",
       "      <td>54900</td>\n",
       "      <td>1.423712</td>\n",
       "      <td>12.654294</td>\n",
       "      <td>132.483994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0.270890</td>\n",
       "      <td>6021</td>\n",
       "      <td>5.347247</td>\n",
       "      <td>60.322857</td>\n",
       "      <td>17.998270</td>\n",
       "      <td>65.577110</td>\n",
       "      <td>26.951323</td>\n",
       "      <td>328.263489</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>27.657429</td>\n",
       "      <td>0.469484</td>\n",
       "      <td>0.595673</td>\n",
       "      <td>1.641759</td>\n",
       "      <td>0.271182</td>\n",
       "      <td>54145</td>\n",
       "      <td>37200</td>\n",
       "      <td>1.873950</td>\n",
       "      <td>11.053498</td>\n",
       "      <td>84.275307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0.268333</td>\n",
       "      <td>5214</td>\n",
       "      <td>4.740000</td>\n",
       "      <td>62.240177</td>\n",
       "      <td>17.185001</td>\n",
       "      <td>64.233215</td>\n",
       "      <td>25.228035</td>\n",
       "      <td>327.084381</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>26.496277</td>\n",
       "      <td>0.466510</td>\n",
       "      <td>0.631739</td>\n",
       "      <td>1.674300</td>\n",
       "      <td>0.274557</td>\n",
       "      <td>49725</td>\n",
       "      <td>35263</td>\n",
       "      <td>1.591575</td>\n",
       "      <td>9.179589</td>\n",
       "      <td>82.861610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0.266154</td>\n",
       "      <td>6654</td>\n",
       "      <td>6.472763</td>\n",
       "      <td>51.220791</td>\n",
       "      <td>20.363977</td>\n",
       "      <td>64.837860</td>\n",
       "      <td>29.682877</td>\n",
       "      <td>313.615784</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>20.848322</td>\n",
       "      <td>0.376367</td>\n",
       "      <td>0.537078</td>\n",
       "      <td>1.352859</td>\n",
       "      <td>0.372806</td>\n",
       "      <td>41247</td>\n",
       "      <td>24821</td>\n",
       "      <td>2.153398</td>\n",
       "      <td>9.223431</td>\n",
       "      <td>79.882202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0.280247</td>\n",
       "      <td>4656</td>\n",
       "      <td>4.409091</td>\n",
       "      <td>65.864754</td>\n",
       "      <td>25.382971</td>\n",
       "      <td>75.770134</td>\n",
       "      <td>37.738964</td>\n",
       "      <td>406.449707</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>31.929028</td>\n",
       "      <td>0.476181</td>\n",
       "      <td>0.513252</td>\n",
       "      <td>1.713084</td>\n",
       "      <td>0.264776</td>\n",
       "      <td>51521</td>\n",
       "      <td>32936</td>\n",
       "      <td>1.183829</td>\n",
       "      <td>10.288476</td>\n",
       "      <td>94.894691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0.244770</td>\n",
       "      <td>7629</td>\n",
       "      <td>7.553465</td>\n",
       "      <td>59.778351</td>\n",
       "      <td>21.099335</td>\n",
       "      <td>69.553505</td>\n",
       "      <td>32.602047</td>\n",
       "      <td>367.868591</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>26.309126</td>\n",
       "      <td>0.426311</td>\n",
       "      <td>0.482346</td>\n",
       "      <td>1.543422</td>\n",
       "      <td>0.311078</td>\n",
       "      <td>45363</td>\n",
       "      <td>27179</td>\n",
       "      <td>3.045509</td>\n",
       "      <td>11.729836</td>\n",
       "      <td>126.983620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0.297397</td>\n",
       "      <td>3213</td>\n",
       "      <td>3.077586</td>\n",
       "      <td>59.083771</td>\n",
       "      <td>24.588932</td>\n",
       "      <td>72.064430</td>\n",
       "      <td>34.771881</td>\n",
       "      <td>344.486908</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>24.858427</td>\n",
       "      <td>0.414168</td>\n",
       "      <td>0.526408</td>\n",
       "      <td>1.437900</td>\n",
       "      <td>0.335748</td>\n",
       "      <td>46039</td>\n",
       "      <td>28187</td>\n",
       "      <td>0.868613</td>\n",
       "      <td>9.812569</td>\n",
       "      <td>49.159668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0.272937</td>\n",
       "      <td>7083</td>\n",
       "      <td>5.922241</td>\n",
       "      <td>61.436592</td>\n",
       "      <td>16.679310</td>\n",
       "      <td>63.519474</td>\n",
       "      <td>26.158899</td>\n",
       "      <td>344.638031</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>29.176376</td>\n",
       "      <td>0.457917</td>\n",
       "      <td>0.619321</td>\n",
       "      <td>1.648766</td>\n",
       "      <td>0.282615</td>\n",
       "      <td>58460</td>\n",
       "      <td>40821</td>\n",
       "      <td>1.663848</td>\n",
       "      <td>10.145830</td>\n",
       "      <td>95.600388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>7242</td>\n",
       "      <td>6.137288</td>\n",
       "      <td>63.979141</td>\n",
       "      <td>32.552345</td>\n",
       "      <td>80.275597</td>\n",
       "      <td>47.583973</td>\n",
       "      <td>484.311188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>27.271307</td>\n",
       "      <td>0.354912</td>\n",
       "      <td>0.430300</td>\n",
       "      <td>1.299548</td>\n",
       "      <td>0.402447</td>\n",
       "      <td>59261</td>\n",
       "      <td>30883</td>\n",
       "      <td>1.299946</td>\n",
       "      <td>10.352946</td>\n",
       "      <td>99.850830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0.272850</td>\n",
       "      <td>3657</td>\n",
       "      <td>3.271020</td>\n",
       "      <td>58.047604</td>\n",
       "      <td>29.810575</td>\n",
       "      <td>76.620857</td>\n",
       "      <td>42.974201</td>\n",
       "      <td>443.180634</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>20.757835</td>\n",
       "      <td>0.359795</td>\n",
       "      <td>0.457270</td>\n",
       "      <td>1.314404</td>\n",
       "      <td>0.400998</td>\n",
       "      <td>52247</td>\n",
       "      <td>28100</td>\n",
       "      <td>0.604963</td>\n",
       "      <td>6.817170</td>\n",
       "      <td>59.627033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.275053</td>\n",
       "      <td>6645</td>\n",
       "      <td>5.738342</td>\n",
       "      <td>62.431053</td>\n",
       "      <td>17.797129</td>\n",
       "      <td>65.079956</td>\n",
       "      <td>27.385105</td>\n",
       "      <td>435.087860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>28.520277</td>\n",
       "      <td>0.420562</td>\n",
       "      <td>0.602618</td>\n",
       "      <td>1.548323</td>\n",
       "      <td>0.323487</td>\n",
       "      <td>52516</td>\n",
       "      <td>35217</td>\n",
       "      <td>1.355569</td>\n",
       "      <td>10.420287</td>\n",
       "      <td>101.883423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0.284168</td>\n",
       "      <td>5985</td>\n",
       "      <td>5.421196</td>\n",
       "      <td>57.861412</td>\n",
       "      <td>17.307076</td>\n",
       "      <td>64.247421</td>\n",
       "      <td>26.151211</td>\n",
       "      <td>365.639160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>26.920105</td>\n",
       "      <td>0.461555</td>\n",
       "      <td>0.605784</td>\n",
       "      <td>1.652247</td>\n",
       "      <td>0.279023</td>\n",
       "      <td>50659</td>\n",
       "      <td>35026</td>\n",
       "      <td>1.469072</td>\n",
       "      <td>9.618235</td>\n",
       "      <td>88.827377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0.241191</td>\n",
       "      <td>9027</td>\n",
       "      <td>7.728596</td>\n",
       "      <td>57.068806</td>\n",
       "      <td>23.015001</td>\n",
       "      <td>71.078712</td>\n",
       "      <td>36.436619</td>\n",
       "      <td>331.332642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>21.463879</td>\n",
       "      <td>0.295795</td>\n",
       "      <td>0.490874</td>\n",
       "      <td>1.151711</td>\n",
       "      <td>0.476626</td>\n",
       "      <td>49223</td>\n",
       "      <td>25218</td>\n",
       "      <td>2.973320</td>\n",
       "      <td>8.470799</td>\n",
       "      <td>122.114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0.256130</td>\n",
       "      <td>9855</td>\n",
       "      <td>8.198835</td>\n",
       "      <td>70.940239</td>\n",
       "      <td>19.423319</td>\n",
       "      <td>70.084961</td>\n",
       "      <td>29.049969</td>\n",
       "      <td>362.650787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>35.135300</td>\n",
       "      <td>0.462363</td>\n",
       "      <td>0.557663</td>\n",
       "      <td>1.643722</td>\n",
       "      <td>0.274110</td>\n",
       "      <td>62972</td>\n",
       "      <td>41732</td>\n",
       "      <td>3.107852</td>\n",
       "      <td>14.776563</td>\n",
       "      <td>143.390717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0.257440</td>\n",
       "      <td>5955</td>\n",
       "      <td>5.826810</td>\n",
       "      <td>67.048485</td>\n",
       "      <td>20.082453</td>\n",
       "      <td>69.544220</td>\n",
       "      <td>29.301947</td>\n",
       "      <td>322.056091</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>30.317440</td>\n",
       "      <td>0.457640</td>\n",
       "      <td>0.555266</td>\n",
       "      <td>1.633766</td>\n",
       "      <td>0.281108</td>\n",
       "      <td>45467</td>\n",
       "      <td>29861</td>\n",
       "      <td>2.447596</td>\n",
       "      <td>11.828444</td>\n",
       "      <td>114.329361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0.263526</td>\n",
       "      <td>4473</td>\n",
       "      <td>4.188202</td>\n",
       "      <td>61.077282</td>\n",
       "      <td>21.304110</td>\n",
       "      <td>70.568481</td>\n",
       "      <td>31.035839</td>\n",
       "      <td>321.116669</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>24.111277</td>\n",
       "      <td>0.400735</td>\n",
       "      <td>0.556070</td>\n",
       "      <td>1.433175</td>\n",
       "      <td>0.348241</td>\n",
       "      <td>45699</td>\n",
       "      <td>28558</td>\n",
       "      <td>1.485060</td>\n",
       "      <td>10.136240</td>\n",
       "      <td>84.243942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0.272726</td>\n",
       "      <td>8682</td>\n",
       "      <td>6.255043</td>\n",
       "      <td>59.589523</td>\n",
       "      <td>35.983669</td>\n",
       "      <td>78.955818</td>\n",
       "      <td>48.305565</td>\n",
       "      <td>315.770508</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>19.429600</td>\n",
       "      <td>0.256911</td>\n",
       "      <td>0.317129</td>\n",
       "      <td>0.995836</td>\n",
       "      <td>0.538398</td>\n",
       "      <td>81782</td>\n",
       "      <td>30928</td>\n",
       "      <td>1.727761</td>\n",
       "      <td>7.782355</td>\n",
       "      <td>85.096565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0.275885</td>\n",
       "      <td>5583</td>\n",
       "      <td>4.637043</td>\n",
       "      <td>55.512218</td>\n",
       "      <td>29.745745</td>\n",
       "      <td>76.926392</td>\n",
       "      <td>43.368145</td>\n",
       "      <td>412.668671</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>20.749704</td>\n",
       "      <td>0.303634</td>\n",
       "      <td>0.461062</td>\n",
       "      <td>1.136022</td>\n",
       "      <td>0.473767</td>\n",
       "      <td>55107</td>\n",
       "      <td>27450</td>\n",
       "      <td>1.007035</td>\n",
       "      <td>8.043703</td>\n",
       "      <td>66.752525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0.258231</td>\n",
       "      <td>4467</td>\n",
       "      <td>3.904720</td>\n",
       "      <td>64.331688</td>\n",
       "      <td>27.118477</td>\n",
       "      <td>77.197868</td>\n",
       "      <td>39.539330</td>\n",
       "      <td>388.664093</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>22.409678</td>\n",
       "      <td>0.347095</td>\n",
       "      <td>0.516769</td>\n",
       "      <td>1.314400</td>\n",
       "      <td>0.416743</td>\n",
       "      <td>50295</td>\n",
       "      <td>28391</td>\n",
       "      <td>0.952655</td>\n",
       "      <td>6.973385</td>\n",
       "      <td>67.370720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0.267715</td>\n",
       "      <td>3966</td>\n",
       "      <td>4.157233</td>\n",
       "      <td>58.267696</td>\n",
       "      <td>20.802584</td>\n",
       "      <td>67.550880</td>\n",
       "      <td>29.784599</td>\n",
       "      <td>312.048431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>25.003973</td>\n",
       "      <td>0.408632</td>\n",
       "      <td>0.551740</td>\n",
       "      <td>1.448616</td>\n",
       "      <td>0.338521</td>\n",
       "      <td>37061</td>\n",
       "      <td>23243</td>\n",
       "      <td>1.400424</td>\n",
       "      <td>10.220130</td>\n",
       "      <td>72.669693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>5</td>\n",
       "      <td>0.329437</td>\n",
       "      <td>1830</td>\n",
       "      <td>1.910230</td>\n",
       "      <td>103.110023</td>\n",
       "      <td>2.441796</td>\n",
       "      <td>31.855747</td>\n",
       "      <td>3.408286</td>\n",
       "      <td>244.729889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>64.144127</td>\n",
       "      <td>0.485064</td>\n",
       "      <td>0.939729</td>\n",
       "      <td>1.716551</td>\n",
       "      <td>0.259840</td>\n",
       "      <td>28992</td>\n",
       "      <td>27735</td>\n",
       "      <td>1.014975</td>\n",
       "      <td>8.306499</td>\n",
       "      <td>114.691544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>5</td>\n",
       "      <td>0.327266</td>\n",
       "      <td>1524</td>\n",
       "      <td>1.886139</td>\n",
       "      <td>99.594414</td>\n",
       "      <td>2.430475</td>\n",
       "      <td>31.958038</td>\n",
       "      <td>3.574899</td>\n",
       "      <td>306.612122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>71.786087</td>\n",
       "      <td>0.570384</td>\n",
       "      <td>0.934521</td>\n",
       "      <td>1.928077</td>\n",
       "      <td>0.179066</td>\n",
       "      <td>24221</td>\n",
       "      <td>23246</td>\n",
       "      <td>0.984496</td>\n",
       "      <td>8.789768</td>\n",
       "      <td>115.527832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>5</td>\n",
       "      <td>0.326804</td>\n",
       "      <td>2055</td>\n",
       "      <td>2.059118</td>\n",
       "      <td>101.488953</td>\n",
       "      <td>3.441503</td>\n",
       "      <td>37.879147</td>\n",
       "      <td>5.019998</td>\n",
       "      <td>279.860321</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>79.448914</td>\n",
       "      <td>0.555202</td>\n",
       "      <td>0.920165</td>\n",
       "      <td>2.001867</td>\n",
       "      <td>0.192895</td>\n",
       "      <td>36577</td>\n",
       "      <td>34538</td>\n",
       "      <td>0.984195</td>\n",
       "      <td>16.416008</td>\n",
       "      <td>112.279274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5</td>\n",
       "      <td>0.281853</td>\n",
       "      <td>2259</td>\n",
       "      <td>2.728261</td>\n",
       "      <td>94.722122</td>\n",
       "      <td>2.801919</td>\n",
       "      <td>33.992092</td>\n",
       "      <td>4.483671</td>\n",
       "      <td>299.014313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>89.089226</td>\n",
       "      <td>0.659065</td>\n",
       "      <td>0.926328</td>\n",
       "      <td>2.240377</td>\n",
       "      <td>0.111974</td>\n",
       "      <td>29093</td>\n",
       "      <td>28092</td>\n",
       "      <td>1.328042</td>\n",
       "      <td>17.803328</td>\n",
       "      <td>123.132950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>5</td>\n",
       "      <td>0.328585</td>\n",
       "      <td>1491</td>\n",
       "      <td>1.859102</td>\n",
       "      <td>96.725357</td>\n",
       "      <td>2.764606</td>\n",
       "      <td>33.592789</td>\n",
       "      <td>3.633090</td>\n",
       "      <td>261.075195</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>63.858864</td>\n",
       "      <td>0.558524</td>\n",
       "      <td>0.928956</td>\n",
       "      <td>1.946990</td>\n",
       "      <td>0.189504</td>\n",
       "      <td>23456</td>\n",
       "      <td>22351</td>\n",
       "      <td>0.990040</td>\n",
       "      <td>11.793193</td>\n",
       "      <td>96.279366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>5</td>\n",
       "      <td>0.326100</td>\n",
       "      <td>1896</td>\n",
       "      <td>2.159453</td>\n",
       "      <td>90.303566</td>\n",
       "      <td>3.006544</td>\n",
       "      <td>35.265091</td>\n",
       "      <td>4.647409</td>\n",
       "      <td>284.565094</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>93.792725</td>\n",
       "      <td>0.705243</td>\n",
       "      <td>0.923688</td>\n",
       "      <td>2.339477</td>\n",
       "      <td>0.083441</td>\n",
       "      <td>35401</td>\n",
       "      <td>33949</td>\n",
       "      <td>0.984424</td>\n",
       "      <td>15.575408</td>\n",
       "      <td>114.775879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>5</td>\n",
       "      <td>0.326406</td>\n",
       "      <td>1605</td>\n",
       "      <td>1.892689</td>\n",
       "      <td>98.245056</td>\n",
       "      <td>2.721795</td>\n",
       "      <td>33.763157</td>\n",
       "      <td>3.749259</td>\n",
       "      <td>300.403625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>67.187927</td>\n",
       "      <td>0.585655</td>\n",
       "      <td>0.928711</td>\n",
       "      <td>1.887980</td>\n",
       "      <td>0.167005</td>\n",
       "      <td>27494</td>\n",
       "      <td>26293</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.555964</td>\n",
       "      <td>108.876099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>5</td>\n",
       "      <td>0.327938</td>\n",
       "      <td>1827</td>\n",
       "      <td>2.052809</td>\n",
       "      <td>88.693619</td>\n",
       "      <td>3.378924</td>\n",
       "      <td>35.093769</td>\n",
       "      <td>4.209288</td>\n",
       "      <td>268.998718</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>75.489136</td>\n",
       "      <td>0.665434</td>\n",
       "      <td>0.917840</td>\n",
       "      <td>2.251300</td>\n",
       "      <td>0.107785</td>\n",
       "      <td>34575</td>\n",
       "      <td>32943</td>\n",
       "      <td>0.987034</td>\n",
       "      <td>14.049045</td>\n",
       "      <td>101.296570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>5</td>\n",
       "      <td>0.325523</td>\n",
       "      <td>1758</td>\n",
       "      <td>2.186567</td>\n",
       "      <td>87.648514</td>\n",
       "      <td>4.793053</td>\n",
       "      <td>38.737522</td>\n",
       "      <td>5.288894</td>\n",
       "      <td>337.040741</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>64.772133</td>\n",
       "      <td>0.620123</td>\n",
       "      <td>0.899450</td>\n",
       "      <td>2.070889</td>\n",
       "      <td>0.138897</td>\n",
       "      <td>26719</td>\n",
       "      <td>25048</td>\n",
       "      <td>1.075229</td>\n",
       "      <td>11.496485</td>\n",
       "      <td>126.448235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>5</td>\n",
       "      <td>0.321219</td>\n",
       "      <td>2151</td>\n",
       "      <td>2.231328</td>\n",
       "      <td>96.395393</td>\n",
       "      <td>3.166469</td>\n",
       "      <td>35.764194</td>\n",
       "      <td>4.839370</td>\n",
       "      <td>285.177185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>78.218842</td>\n",
       "      <td>0.603720</td>\n",
       "      <td>0.920634</td>\n",
       "      <td>2.066045</td>\n",
       "      <td>0.152318</td>\n",
       "      <td>36895</td>\n",
       "      <td>35055</td>\n",
       "      <td>1.043668</td>\n",
       "      <td>11.709446</td>\n",
       "      <td>115.713966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5</td>\n",
       "      <td>0.326276</td>\n",
       "      <td>1602</td>\n",
       "      <td>2.069767</td>\n",
       "      <td>88.427338</td>\n",
       "      <td>2.994148</td>\n",
       "      <td>33.049721</td>\n",
       "      <td>4.109481</td>\n",
       "      <td>335.458649</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>78.597244</td>\n",
       "      <td>0.683234</td>\n",
       "      <td>0.920057</td>\n",
       "      <td>2.190032</td>\n",
       "      <td>0.096248</td>\n",
       "      <td>26679</td>\n",
       "      <td>25568</td>\n",
       "      <td>0.967391</td>\n",
       "      <td>9.548928</td>\n",
       "      <td>126.519577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>5</td>\n",
       "      <td>0.327303</td>\n",
       "      <td>1953</td>\n",
       "      <td>2.160398</td>\n",
       "      <td>93.071259</td>\n",
       "      <td>3.332178</td>\n",
       "      <td>36.207199</td>\n",
       "      <td>4.390880</td>\n",
       "      <td>274.148376</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>72.242599</td>\n",
       "      <td>0.628074</td>\n",
       "      <td>0.916472</td>\n",
       "      <td>2.129619</td>\n",
       "      <td>0.133765</td>\n",
       "      <td>33783</td>\n",
       "      <td>32077</td>\n",
       "      <td>0.987860</td>\n",
       "      <td>11.349506</td>\n",
       "      <td>102.957901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>5</td>\n",
       "      <td>0.324674</td>\n",
       "      <td>1854</td>\n",
       "      <td>2.111617</td>\n",
       "      <td>96.540955</td>\n",
       "      <td>3.524834</td>\n",
       "      <td>38.277061</td>\n",
       "      <td>4.575498</td>\n",
       "      <td>287.663452</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>75.429817</td>\n",
       "      <td>0.632164</td>\n",
       "      <td>0.915942</td>\n",
       "      <td>2.198740</td>\n",
       "      <td>0.130447</td>\n",
       "      <td>32075</td>\n",
       "      <td>30445</td>\n",
       "      <td>1.018122</td>\n",
       "      <td>13.339722</td>\n",
       "      <td>112.174835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>5</td>\n",
       "      <td>0.324694</td>\n",
       "      <td>1488</td>\n",
       "      <td>1.823529</td>\n",
       "      <td>97.722816</td>\n",
       "      <td>3.490817</td>\n",
       "      <td>36.636883</td>\n",
       "      <td>4.156250</td>\n",
       "      <td>296.023438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>55.487644</td>\n",
       "      <td>0.525817</td>\n",
       "      <td>0.916817</td>\n",
       "      <td>1.833374</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>23246</td>\n",
       "      <td>21874</td>\n",
       "      <td>0.946565</td>\n",
       "      <td>8.772933</td>\n",
       "      <td>106.374138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>5</td>\n",
       "      <td>0.327418</td>\n",
       "      <td>1578</td>\n",
       "      <td>1.887560</td>\n",
       "      <td>95.542412</td>\n",
       "      <td>3.086411</td>\n",
       "      <td>34.290192</td>\n",
       "      <td>3.531146</td>\n",
       "      <td>267.528473</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>64.755554</td>\n",
       "      <td>0.587546</td>\n",
       "      <td>0.927388</td>\n",
       "      <td>1.949812</td>\n",
       "      <td>0.165660</td>\n",
       "      <td>26878</td>\n",
       "      <td>25627</td>\n",
       "      <td>0.974074</td>\n",
       "      <td>9.476419</td>\n",
       "      <td>96.450935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>5</td>\n",
       "      <td>0.324889</td>\n",
       "      <td>1818</td>\n",
       "      <td>2.283920</td>\n",
       "      <td>91.204094</td>\n",
       "      <td>4.084125</td>\n",
       "      <td>36.413788</td>\n",
       "      <td>4.633706</td>\n",
       "      <td>295.940063</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>59.058659</td>\n",
       "      <td>0.602896</td>\n",
       "      <td>0.911252</td>\n",
       "      <td>2.027760</td>\n",
       "      <td>0.152561</td>\n",
       "      <td>25256</td>\n",
       "      <td>23815</td>\n",
       "      <td>1.145558</td>\n",
       "      <td>11.370978</td>\n",
       "      <td>118.635445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>5</td>\n",
       "      <td>0.325060</td>\n",
       "      <td>1908</td>\n",
       "      <td>2.124722</td>\n",
       "      <td>93.248558</td>\n",
       "      <td>2.599405</td>\n",
       "      <td>32.281033</td>\n",
       "      <td>3.685436</td>\n",
       "      <td>279.206268</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>79.007439</td>\n",
       "      <td>0.657976</td>\n",
       "      <td>0.931927</td>\n",
       "      <td>2.206697</td>\n",
       "      <td>0.112917</td>\n",
       "      <td>34459</td>\n",
       "      <td>33162</td>\n",
       "      <td>1.017600</td>\n",
       "      <td>9.466627</td>\n",
       "      <td>106.758987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>5</td>\n",
       "      <td>0.325029</td>\n",
       "      <td>1815</td>\n",
       "      <td>2.021158</td>\n",
       "      <td>92.299522</td>\n",
       "      <td>10.997183</td>\n",
       "      <td>56.123600</td>\n",
       "      <td>12.546728</td>\n",
       "      <td>477.698029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>81.431961</td>\n",
       "      <td>0.666316</td>\n",
       "      <td>0.832733</td>\n",
       "      <td>2.210949</td>\n",
       "      <td>0.107409</td>\n",
       "      <td>37491</td>\n",
       "      <td>33581</td>\n",
       "      <td>0.761965</td>\n",
       "      <td>12.036510</td>\n",
       "      <td>110.562927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>5</td>\n",
       "      <td>0.320871</td>\n",
       "      <td>1932</td>\n",
       "      <td>2.090909</td>\n",
       "      <td>90.313362</td>\n",
       "      <td>11.147587</td>\n",
       "      <td>54.607380</td>\n",
       "      <td>12.189240</td>\n",
       "      <td>478.361023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>77.288742</td>\n",
       "      <td>0.654494</td>\n",
       "      <td>0.830556</td>\n",
       "      <td>2.180774</td>\n",
       "      <td>0.115616</td>\n",
       "      <td>39129</td>\n",
       "      <td>34914</td>\n",
       "      <td>0.691729</td>\n",
       "      <td>11.754579</td>\n",
       "      <td>96.310394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>5</td>\n",
       "      <td>0.325034</td>\n",
       "      <td>2280</td>\n",
       "      <td>2.039356</td>\n",
       "      <td>90.240158</td>\n",
       "      <td>26.476332</td>\n",
       "      <td>76.999329</td>\n",
       "      <td>26.695816</td>\n",
       "      <td>502.114563</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>57.892426</td>\n",
       "      <td>0.538403</td>\n",
       "      <td>0.633065</td>\n",
       "      <td>1.755866</td>\n",
       "      <td>0.209352</td>\n",
       "      <td>56973</td>\n",
       "      <td>41947</td>\n",
       "      <td>0.440324</td>\n",
       "      <td>5.906392</td>\n",
       "      <td>79.529709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>5</td>\n",
       "      <td>0.329047</td>\n",
       "      <td>2223</td>\n",
       "      <td>1.974245</td>\n",
       "      <td>91.014145</td>\n",
       "      <td>19.715767</td>\n",
       "      <td>67.531876</td>\n",
       "      <td>20.068239</td>\n",
       "      <td>607.079163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>61.842533</td>\n",
       "      <td>0.551711</td>\n",
       "      <td>0.723281</td>\n",
       "      <td>1.894633</td>\n",
       "      <td>0.197115</td>\n",
       "      <td>54314</td>\n",
       "      <td>43530</td>\n",
       "      <td>0.380195</td>\n",
       "      <td>7.953485</td>\n",
       "      <td>72.295876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>5</td>\n",
       "      <td>0.325958</td>\n",
       "      <td>1854</td>\n",
       "      <td>1.993548</td>\n",
       "      <td>84.701866</td>\n",
       "      <td>3.718848</td>\n",
       "      <td>34.255978</td>\n",
       "      <td>4.663664</td>\n",
       "      <td>290.396759</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>64.947853</td>\n",
       "      <td>0.631669</td>\n",
       "      <td>0.912085</td>\n",
       "      <td>1.967245</td>\n",
       "      <td>0.132457</td>\n",
       "      <td>35983</td>\n",
       "      <td>34076</td>\n",
       "      <td>0.836265</td>\n",
       "      <td>5.979109</td>\n",
       "      <td>85.435814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>5</td>\n",
       "      <td>0.317268</td>\n",
       "      <td>3156</td>\n",
       "      <td>3.415584</td>\n",
       "      <td>81.582077</td>\n",
       "      <td>2.912717</td>\n",
       "      <td>40.511196</td>\n",
       "      <td>6.394245</td>\n",
       "      <td>145.427307</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>55.905907</td>\n",
       "      <td>0.582396</td>\n",
       "      <td>0.868049</td>\n",
       "      <td>1.847106</td>\n",
       "      <td>0.168466</td>\n",
       "      <td>33926</td>\n",
       "      <td>30867</td>\n",
       "      <td>1.628483</td>\n",
       "      <td>5.019736</td>\n",
       "      <td>63.298687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>5</td>\n",
       "      <td>0.315875</td>\n",
       "      <td>3291</td>\n",
       "      <td>3.064246</td>\n",
       "      <td>88.589043</td>\n",
       "      <td>2.504325</td>\n",
       "      <td>39.202515</td>\n",
       "      <td>5.548408</td>\n",
       "      <td>159.888672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>56.213657</td>\n",
       "      <td>0.528118</td>\n",
       "      <td>0.889973</td>\n",
       "      <td>1.728282</td>\n",
       "      <td>0.216915</td>\n",
       "      <td>41334</td>\n",
       "      <td>37977</td>\n",
       "      <td>1.271147</td>\n",
       "      <td>3.865791</td>\n",
       "      <td>63.336151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>5</td>\n",
       "      <td>0.317674</td>\n",
       "      <td>2712</td>\n",
       "      <td>2.767347</td>\n",
       "      <td>87.885124</td>\n",
       "      <td>5.677615</td>\n",
       "      <td>48.981297</td>\n",
       "      <td>9.692434</td>\n",
       "      <td>235.777710</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>51.301086</td>\n",
       "      <td>0.514803</td>\n",
       "      <td>0.837093</td>\n",
       "      <td>1.600996</td>\n",
       "      <td>0.230660</td>\n",
       "      <td>35148</td>\n",
       "      <td>30674</td>\n",
       "      <td>0.963753</td>\n",
       "      <td>2.882149</td>\n",
       "      <td>53.959511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>5</td>\n",
       "      <td>0.318309</td>\n",
       "      <td>3000</td>\n",
       "      <td>3.151261</td>\n",
       "      <td>88.040794</td>\n",
       "      <td>5.040944</td>\n",
       "      <td>47.410305</td>\n",
       "      <td>9.454280</td>\n",
       "      <td>268.489197</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62.155499</td>\n",
       "      <td>0.586186</td>\n",
       "      <td>0.838734</td>\n",
       "      <td>1.869461</td>\n",
       "      <td>0.165214</td>\n",
       "      <td>37130</td>\n",
       "      <td>33072</td>\n",
       "      <td>1.101322</td>\n",
       "      <td>4.540793</td>\n",
       "      <td>73.999702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>5</td>\n",
       "      <td>0.321784</td>\n",
       "      <td>2109</td>\n",
       "      <td>2.292391</td>\n",
       "      <td>87.076225</td>\n",
       "      <td>4.710301</td>\n",
       "      <td>45.026558</td>\n",
       "      <td>8.132174</td>\n",
       "      <td>194.335556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>55.446995</td>\n",
       "      <td>0.579606</td>\n",
       "      <td>0.853446</td>\n",
       "      <td>1.750493</td>\n",
       "      <td>0.173076</td>\n",
       "      <td>34124</td>\n",
       "      <td>30591</td>\n",
       "      <td>1.011511</td>\n",
       "      <td>2.557078</td>\n",
       "      <td>41.734322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>5</td>\n",
       "      <td>0.312539</td>\n",
       "      <td>2706</td>\n",
       "      <td>2.297114</td>\n",
       "      <td>92.992523</td>\n",
       "      <td>4.442750</td>\n",
       "      <td>39.690025</td>\n",
       "      <td>6.023984</td>\n",
       "      <td>273.661011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>71.508949</td>\n",
       "      <td>0.562739</td>\n",
       "      <td>0.900247</td>\n",
       "      <td>1.944592</td>\n",
       "      <td>0.186766</td>\n",
       "      <td>52499</td>\n",
       "      <td>48803</td>\n",
       "      <td>0.850141</td>\n",
       "      <td>11.819439</td>\n",
       "      <td>86.854179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>5</td>\n",
       "      <td>0.327713</td>\n",
       "      <td>1929</td>\n",
       "      <td>2.078664</td>\n",
       "      <td>86.216850</td>\n",
       "      <td>2.827127</td>\n",
       "      <td>32.561413</td>\n",
       "      <td>3.942419</td>\n",
       "      <td>231.265396</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>70.938370</td>\n",
       "      <td>0.665162</td>\n",
       "      <td>0.924062</td>\n",
       "      <td>2.079872</td>\n",
       "      <td>0.109220</td>\n",
       "      <td>37422</td>\n",
       "      <td>35799</td>\n",
       "      <td>1.003120</td>\n",
       "      <td>6.599238</td>\n",
       "      <td>76.926056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>5</td>\n",
       "      <td>0.328494</td>\n",
       "      <td>1701</td>\n",
       "      <td>1.950688</td>\n",
       "      <td>90.048561</td>\n",
       "      <td>3.401406</td>\n",
       "      <td>34.405643</td>\n",
       "      <td>4.510672</td>\n",
       "      <td>278.239990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>71.070496</td>\n",
       "      <td>0.635851</td>\n",
       "      <td>0.917536</td>\n",
       "      <td>2.084897</td>\n",
       "      <td>0.128606</td>\n",
       "      <td>31792</td>\n",
       "      <td>30208</td>\n",
       "      <td>0.948161</td>\n",
       "      <td>8.564831</td>\n",
       "      <td>87.500160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Jenis  Solidity_Inti  Keliling_Inti  KelilingNormal_I  Granularity_Inti  \\\n",
       "0        1       0.295574           2235          2.337866        104.162460   \n",
       "1        1       0.276264           4125          3.669929         63.401608   \n",
       "2        1       0.274679           7965          6.926087         65.156616   \n",
       "3        1       0.263279           8115          7.431319         66.084785   \n",
       "4        1       0.228109           9930          8.316583         66.808174   \n",
       "5        1       0.242267           6978          6.533708         53.952385   \n",
       "6        1       0.242138           7173          5.766077         63.488342   \n",
       "7        1       0.273889           4785          4.139273         66.707657   \n",
       "8        1       0.268239           5421          5.346154         60.061481   \n",
       "9        1       0.272751           6276          5.664260         58.653553   \n",
       "10       1       0.253547          12681          9.149351         68.033195   \n",
       "11       1       0.270890           6021          5.347247         60.322857   \n",
       "12       1       0.268333           5214          4.740000         62.240177   \n",
       "13       1       0.266154           6654          6.472763         51.220791   \n",
       "14       1       0.280247           4656          4.409091         65.864754   \n",
       "15       1       0.244770           7629          7.553465         59.778351   \n",
       "16       1       0.297397           3213          3.077586         59.083771   \n",
       "17       1       0.272937           7083          5.922241         61.436592   \n",
       "18       1       0.245882           7242          6.137288         63.979141   \n",
       "19       1       0.272850           3657          3.271020         58.047604   \n",
       "20       1       0.275053           6645          5.738342         62.431053   \n",
       "21       1       0.284168           5985          5.421196         57.861412   \n",
       "22       1       0.241191           9027          7.728596         57.068806   \n",
       "23       1       0.256130           9855          8.198835         70.940239   \n",
       "24       1       0.257440           5955          5.826810         67.048485   \n",
       "25       1       0.263526           4473          4.188202         61.077282   \n",
       "26       1       0.272726           8682          6.255043         59.589523   \n",
       "27       1       0.275885           5583          4.637043         55.512218   \n",
       "28       1       0.258231           4467          3.904720         64.331688   \n",
       "29       1       0.267715           3966          4.157233         58.267696   \n",
       "..     ...            ...            ...               ...               ...   \n",
       "139      5       0.329437           1830          1.910230        103.110023   \n",
       "140      5       0.327266           1524          1.886139         99.594414   \n",
       "141      5       0.326804           2055          2.059118        101.488953   \n",
       "142      5       0.281853           2259          2.728261         94.722122   \n",
       "143      5       0.328585           1491          1.859102         96.725357   \n",
       "144      5       0.326100           1896          2.159453         90.303566   \n",
       "145      5       0.326406           1605          1.892689         98.245056   \n",
       "146      5       0.327938           1827          2.052809         88.693619   \n",
       "147      5       0.325523           1758          2.186567         87.648514   \n",
       "148      5       0.321219           2151          2.231328         96.395393   \n",
       "149      5       0.326276           1602          2.069767         88.427338   \n",
       "150      5       0.327303           1953          2.160398         93.071259   \n",
       "151      5       0.324674           1854          2.111617         96.540955   \n",
       "152      5       0.324694           1488          1.823529         97.722816   \n",
       "153      5       0.327418           1578          1.887560         95.542412   \n",
       "154      5       0.324889           1818          2.283920         91.204094   \n",
       "155      5       0.325060           1908          2.124722         93.248558   \n",
       "156      5       0.325029           1815          2.021158         92.299522   \n",
       "157      5       0.320871           1932          2.090909         90.313362   \n",
       "158      5       0.325034           2280          2.039356         90.240158   \n",
       "159      5       0.329047           2223          1.974245         91.014145   \n",
       "160      5       0.325958           1854          1.993548         84.701866   \n",
       "161      5       0.317268           3156          3.415584         81.582077   \n",
       "162      5       0.315875           3291          3.064246         88.589043   \n",
       "163      5       0.317674           2712          2.767347         87.885124   \n",
       "164      5       0.318309           3000          3.151261         88.040794   \n",
       "165      5       0.321784           2109          2.292391         87.076225   \n",
       "166      5       0.312539           2706          2.297114         92.992523   \n",
       "167      5       0.327713           1929          2.078664         86.216850   \n",
       "168      5       0.328494           1701          1.950688         90.048561   \n",
       "\n",
       "      RerataGP  GranularityP   RerataBP    KontrasP  Solidity_Inti.1  \\\n",
       "0    13.242218     67.830696  20.572479  411.989685              NaN   \n",
       "1    22.188545     71.498077  32.688419  423.807465              NaN   \n",
       "2    18.550638     69.066055  28.637329  373.516144              NaN   \n",
       "3    18.314508     66.335373  28.886127  411.183746              NaN   \n",
       "4    25.652452     76.921638  39.452927  449.707581              NaN   \n",
       "5    22.601267     69.807327  34.568325  315.389404              NaN   \n",
       "6    29.484987     79.742218  42.271564  481.051636              NaN   \n",
       "7    26.333082     78.260277  36.513149  429.702301              NaN   \n",
       "8    21.753721     71.431885  33.718357  385.725739              NaN   \n",
       "9    25.858793     74.579300  39.881699  343.112488              NaN   \n",
       "10   23.991095     73.897148  34.626331  525.832275              NaN   \n",
       "11   17.998270     65.577110  26.951323  328.263489              NaN   \n",
       "12   17.185001     64.233215  25.228035  327.084381              NaN   \n",
       "13   20.363977     64.837860  29.682877  313.615784              NaN   \n",
       "14   25.382971     75.770134  37.738964  406.449707              NaN   \n",
       "15   21.099335     69.553505  32.602047  367.868591              NaN   \n",
       "16   24.588932     72.064430  34.771881  344.486908              NaN   \n",
       "17   16.679310     63.519474  26.158899  344.638031              NaN   \n",
       "18   32.552345     80.275597  47.583973  484.311188              NaN   \n",
       "19   29.810575     76.620857  42.974201  443.180634              NaN   \n",
       "20   17.797129     65.079956  27.385105  435.087860              NaN   \n",
       "21   17.307076     64.247421  26.151211  365.639160              NaN   \n",
       "22   23.015001     71.078712  36.436619  331.332642              NaN   \n",
       "23   19.423319     70.084961  29.049969  362.650787              NaN   \n",
       "24   20.082453     69.544220  29.301947  322.056091              NaN   \n",
       "25   21.304110     70.568481  31.035839  321.116669              NaN   \n",
       "26   35.983669     78.955818  48.305565  315.770508              NaN   \n",
       "27   29.745745     76.926392  43.368145  412.668671              NaN   \n",
       "28   27.118477     77.197868  39.539330  388.664093              NaN   \n",
       "29   20.802584     67.550880  29.784599  312.048431              NaN   \n",
       "..         ...           ...        ...         ...              ...   \n",
       "139   2.441796     31.855747   3.408286  244.729889              NaN   \n",
       "140   2.430475     31.958038   3.574899  306.612122              NaN   \n",
       "141   3.441503     37.879147   5.019998  279.860321              NaN   \n",
       "142   2.801919     33.992092   4.483671  299.014313              NaN   \n",
       "143   2.764606     33.592789   3.633090  261.075195              NaN   \n",
       "144   3.006544     35.265091   4.647409  284.565094              NaN   \n",
       "145   2.721795     33.763157   3.749259  300.403625              NaN   \n",
       "146   3.378924     35.093769   4.209288  268.998718              NaN   \n",
       "147   4.793053     38.737522   5.288894  337.040741              NaN   \n",
       "148   3.166469     35.764194   4.839370  285.177185              NaN   \n",
       "149   2.994148     33.049721   4.109481  335.458649              NaN   \n",
       "150   3.332178     36.207199   4.390880  274.148376              NaN   \n",
       "151   3.524834     38.277061   4.575498  287.663452              NaN   \n",
       "152   3.490817     36.636883   4.156250  296.023438              NaN   \n",
       "153   3.086411     34.290192   3.531146  267.528473              NaN   \n",
       "154   4.084125     36.413788   4.633706  295.940063              NaN   \n",
       "155   2.599405     32.281033   3.685436  279.206268              NaN   \n",
       "156  10.997183     56.123600  12.546728  477.698029              NaN   \n",
       "157  11.147587     54.607380  12.189240  478.361023              NaN   \n",
       "158  26.476332     76.999329  26.695816  502.114563              NaN   \n",
       "159  19.715767     67.531876  20.068239  607.079163              NaN   \n",
       "160   3.718848     34.255978   4.663664  290.396759              NaN   \n",
       "161   2.912717     40.511196   6.394245  145.427307              NaN   \n",
       "162   2.504325     39.202515   5.548408  159.888672              NaN   \n",
       "163   5.677615     48.981297   9.692434  235.777710              NaN   \n",
       "164   5.040944     47.410305   9.454280  268.489197              NaN   \n",
       "165   4.710301     45.026558   8.132174  194.335556              NaN   \n",
       "166   4.442750     39.690025   6.023984  273.661011              NaN   \n",
       "167   2.827127     32.561413   3.942419  231.265396              NaN   \n",
       "168   3.401406     34.405643   4.510672  278.239990              NaN   \n",
       "\n",
       "         ...       RerataB_Inti  LuasNormal_P   EnergiP  Entropi_Inti  \\\n",
       "0        ...          61.313919      0.391224  0.785876      1.517679   \n",
       "1        ...          26.692270      0.393040  0.577366      1.428150   \n",
       "2        ...          31.138128      0.446200  0.599164      1.648434   \n",
       "3        ...          34.198906      0.469368  0.577987      1.662973   \n",
       "4        ...          27.163689      0.357650  0.475496      1.343836   \n",
       "5        ...          19.595343      0.332632  0.473613      1.230935   \n",
       "6        ...          24.819304      0.330642  0.464251      1.261649   \n",
       "7        ...          25.246431      0.343561  0.523086      1.265952   \n",
       "8        ...          26.141420      0.438259  0.527767      1.593338   \n",
       "9        ...          24.212234      0.348072  0.473379      1.288087   \n",
       "10       ...          32.101109      0.457309  0.522469      1.658149   \n",
       "11       ...          27.657429      0.469484  0.595673      1.641759   \n",
       "12       ...          26.496277      0.466510  0.631739      1.674300   \n",
       "13       ...          20.848322      0.376367  0.537078      1.352859   \n",
       "14       ...          31.929028      0.476181  0.513252      1.713084   \n",
       "15       ...          26.309126      0.426311  0.482346      1.543422   \n",
       "16       ...          24.858427      0.414168  0.526408      1.437900   \n",
       "17       ...          29.176376      0.457917  0.619321      1.648766   \n",
       "18       ...          27.271307      0.354912  0.430300      1.299548   \n",
       "19       ...          20.757835      0.359795  0.457270      1.314404   \n",
       "20       ...          28.520277      0.420562  0.602618      1.548323   \n",
       "21       ...          26.920105      0.461555  0.605784      1.652247   \n",
       "22       ...          21.463879      0.295795  0.490874      1.151711   \n",
       "23       ...          35.135300      0.462363  0.557663      1.643722   \n",
       "24       ...          30.317440      0.457640  0.555266      1.633766   \n",
       "25       ...          24.111277      0.400735  0.556070      1.433175   \n",
       "26       ...          19.429600      0.256911  0.317129      0.995836   \n",
       "27       ...          20.749704      0.303634  0.461062      1.136022   \n",
       "28       ...          22.409678      0.347095  0.516769      1.314400   \n",
       "29       ...          25.003973      0.408632  0.551740      1.448616   \n",
       "..       ...                ...           ...       ...           ...   \n",
       "139      ...          64.144127      0.485064  0.939729      1.716551   \n",
       "140      ...          71.786087      0.570384  0.934521      1.928077   \n",
       "141      ...          79.448914      0.555202  0.920165      2.001867   \n",
       "142      ...          89.089226      0.659065  0.926328      2.240377   \n",
       "143      ...          63.858864      0.558524  0.928956      1.946990   \n",
       "144      ...          93.792725      0.705243  0.923688      2.339477   \n",
       "145      ...          67.187927      0.585655  0.928711      1.887980   \n",
       "146      ...          75.489136      0.665434  0.917840      2.251300   \n",
       "147      ...          64.772133      0.620123  0.899450      2.070889   \n",
       "148      ...          78.218842      0.603720  0.920634      2.066045   \n",
       "149      ...          78.597244      0.683234  0.920057      2.190032   \n",
       "150      ...          72.242599      0.628074  0.916472      2.129619   \n",
       "151      ...          75.429817      0.632164  0.915942      2.198740   \n",
       "152      ...          55.487644      0.525817  0.916817      1.833374   \n",
       "153      ...          64.755554      0.587546  0.927388      1.949812   \n",
       "154      ...          59.058659      0.602896  0.911252      2.027760   \n",
       "155      ...          79.007439      0.657976  0.931927      2.206697   \n",
       "156      ...          81.431961      0.666316  0.832733      2.210949   \n",
       "157      ...          77.288742      0.654494  0.830556      2.180774   \n",
       "158      ...          57.892426      0.538403  0.633065      1.755866   \n",
       "159      ...          61.842533      0.551711  0.723281      1.894633   \n",
       "160      ...          64.947853      0.631669  0.912085      1.967245   \n",
       "161      ...          55.905907      0.582396  0.868049      1.847106   \n",
       "162      ...          56.213657      0.528118  0.889973      1.728282   \n",
       "163      ...          51.301086      0.514803  0.837093      1.600996   \n",
       "164      ...          62.155499      0.586186  0.838734      1.869461   \n",
       "165      ...          55.446995      0.579606  0.853446      1.750493   \n",
       "166      ...          71.508949      0.562739  0.900247      1.944592   \n",
       "167      ...          70.938370      0.665162  0.924062      2.079872   \n",
       "168      ...          71.070496      0.635851  0.917536      2.084897   \n",
       "\n",
       "     Energi_Inti  LuasP  Luas_Inti   KIperKP  RerataG_Inti  Kontras_Inti  \n",
       "0       0.363583  28320      22333  1.041958      8.136516    125.540268  \n",
       "1       0.359131  48863      31025  0.867508      9.076974     71.531929  \n",
       "2       0.293116  54135      36832  2.514205     11.777470    127.077431  \n",
       "3       0.267211  51431      34974  2.064885     13.302699    126.954521  \n",
       "4       0.394241  57700      31828  2.439204     11.433691    147.499649  \n",
       "5       0.428663  44690      23701  2.581576      8.621966     90.372208  \n",
       "6       0.434828  61094      31947  1.166911      9.485422     99.338615  \n",
       "7       0.420945  50622      28686  1.000627     10.557991     79.043915  \n",
       "8       0.302863  44634      28145  1.757782      9.860604     99.393921  \n",
       "9       0.410599  49300      26639  1.874552      9.376661     98.397949  \n",
       "10      0.280668  85727      54900  1.423712     12.654294    132.483994  \n",
       "11      0.271182  54145      37200  1.873950     11.053498     84.275307  \n",
       "12      0.274557  49725      35263  1.591575      9.179589     82.861610  \n",
       "13      0.372806  41247      24821  2.153398      9.223431     79.882202  \n",
       "14      0.264776  51521      32936  1.183829     10.288476     94.894691  \n",
       "15      0.311078  45363      27179  3.045509     11.729836    126.983620  \n",
       "16      0.335748  46039      28187  0.868613      9.812569     49.159668  \n",
       "17      0.282615  58460      40821  1.663848     10.145830     95.600388  \n",
       "18      0.402447  59261      30883  1.299946     10.352946     99.850830  \n",
       "19      0.400998  52247      28100  0.604963      6.817170     59.627033  \n",
       "20      0.323487  52516      35217  1.355569     10.420287    101.883423  \n",
       "21      0.279023  50659      35026  1.469072      9.618235     88.827377  \n",
       "22      0.476626  49223      25218  2.973320      8.470799    122.114700  \n",
       "23      0.274110  62972      41732  3.107852     14.776563    143.390717  \n",
       "24      0.281108  45467      29861  2.447596     11.828444    114.329361  \n",
       "25      0.348241  45699      28558  1.485060     10.136240     84.243942  \n",
       "26      0.538398  81782      30928  1.727761      7.782355     85.096565  \n",
       "27      0.473767  55107      27450  1.007035      8.043703     66.752525  \n",
       "28      0.416743  50295      28391  0.952655      6.973385     67.370720  \n",
       "29      0.338521  37061      23243  1.400424     10.220130     72.669693  \n",
       "..           ...    ...        ...       ...           ...           ...  \n",
       "139     0.259840  28992      27735  1.014975      8.306499    114.691544  \n",
       "140     0.179066  24221      23246  0.984496      8.789768    115.527832  \n",
       "141     0.192895  36577      34538  0.984195     16.416008    112.279274  \n",
       "142     0.111974  29093      28092  1.328042     17.803328    123.132950  \n",
       "143     0.189504  23456      22351  0.990040     11.793193     96.279366  \n",
       "144     0.083441  35401      33949  0.984424     15.575408    114.775879  \n",
       "145     0.167005  27494      26293  1.000000      6.555964    108.876099  \n",
       "146     0.107785  34575      32943  0.987034     14.049045    101.296570  \n",
       "147     0.138897  26719      25048  1.075229     11.496485    126.448235  \n",
       "148     0.152318  36895      35055  1.043668     11.709446    115.713966  \n",
       "149     0.096248  26679      25568  0.967391      9.548928    126.519577  \n",
       "150     0.133765  33783      32077  0.987860     11.349506    102.957901  \n",
       "151     0.130447  32075      30445  1.018122     13.339722    112.174835  \n",
       "152     0.218750  23246      21874  0.946565      8.772933    106.374138  \n",
       "153     0.165660  26878      25627  0.974074      9.476419     96.450935  \n",
       "154     0.152561  25256      23815  1.145558     11.370978    118.635445  \n",
       "155     0.112917  34459      33162  1.017600      9.466627    106.758987  \n",
       "156     0.107409  37491      33581  0.761965     12.036510    110.562927  \n",
       "157     0.115616  39129      34914  0.691729     11.754579     96.310394  \n",
       "158     0.209352  56973      41947  0.440324      5.906392     79.529709  \n",
       "159     0.197115  54314      43530  0.380195      7.953485     72.295876  \n",
       "160     0.132457  35983      34076  0.836265      5.979109     85.435814  \n",
       "161     0.168466  33926      30867  1.628483      5.019736     63.298687  \n",
       "162     0.216915  41334      37977  1.271147      3.865791     63.336151  \n",
       "163     0.230660  35148      30674  0.963753      2.882149     53.959511  \n",
       "164     0.165214  37130      33072  1.101322      4.540793     73.999702  \n",
       "165     0.173076  34124      30591  1.011511      2.557078     41.734322  \n",
       "166     0.186766  52499      48803  0.850141     11.819439     86.854179  \n",
       "167     0.109220  37422      35799  1.003120      6.599238     76.926056  \n",
       "168     0.128606  31792      30208  0.948161      8.564831     87.500160  \n",
       "\n",
       "[169 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumlah fitur:  5\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=10, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0 10  0  2]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=11, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0  9  0  3]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=12, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0  8  0  4]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=13, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0  9  0  3]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=14, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  0  0  1  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0  9  0  3]\n",
      " [ 2  0  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=15, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  0  0  1  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0  8  0  4]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.803921568627451\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=16, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0  9  0  3]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=17, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0  9  0  3]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=18, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0 10  0  2]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=19, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0  9  0  3]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=20, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0  9  0  3]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=21, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0 10  0  2]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=22, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0 10  0  2]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=23, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0 10  0  2]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=24, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  0  0  1  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0  9  0  3]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=25, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0  9  1  2]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=26, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0  9  1  2]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=27, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0 10  0  2]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=28, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  0  0  1  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0 10  0  2]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=29, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  0  0  1  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0 10  0  2]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=30, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0 10  0  2]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=31, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  0  0  1  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0  9  1  2]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=36, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0  9  1  2]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  1  0 20]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=41, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  0  0  1  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0 10  0  2]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  2  0 19]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=46, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  0  0  1  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0 10  0  2]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  2  0 19]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=51, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0 10  0  2]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  2  0 19]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=56, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0 10  0  2]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  2  0 19]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=61, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0 10  0  2]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  2  0 19]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=66, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0 10  0  2]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  2  0 19]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=71, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  0  0  1  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0 10  0  2]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  2  0 19]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=76, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0 10  0  2]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  2  0 19]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "jumlah fitur:  6\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=10, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  5  0 16]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=11, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  0  0  1  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 11  0  1]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  5  0 16]]\n",
      "akurasi uji:  0.7843137254901961\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=12, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 11  1  0]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  5  0 16]]\n",
      "akurasi uji:  0.803921568627451\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=13, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=14, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 11  0  1]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=15, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 11  0  1]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=16, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 10  1  1]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  4  1 16]]\n",
      "akurasi uji:  0.7843137254901961\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=17, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 11  0  1]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=18, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  5  0 16]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=19, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 10  1  1]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  4  1 16]]\n",
      "akurasi uji:  0.7843137254901961\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=20, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 11  0  1]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=21, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  1 16]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=22, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 10  1  1]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  4  1 16]]\n",
      "akurasi uji:  0.7843137254901961\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=23, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[ 9  1  0  1  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=24, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=25, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  1  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=26, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=27, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  1  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=28, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  5  0 16]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=29, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  1  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=30, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  1  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=31, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 11  1  0]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  5  0 16]]\n",
      "akurasi uji:  0.803921568627451\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=36, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  2  0  3  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=41, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=46, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  1  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=51, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=56, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  1  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=61, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  1  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=66, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  1  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=71, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  1  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=76, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 11  1  0]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  4  1 16]]\n",
      "akurasi uji:  0.803921568627451\n",
      "\n",
      "jumlah fitur:  7\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=10, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  0  0  1  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  5  0 16]]\n",
      "akurasi uji:  0.803921568627451\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=11, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  5  0 16]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=12, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  5  0 16]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=13, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=14, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  5  0 16]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=15, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=16, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  5  0 16]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=17, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=18, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  1  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=19, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=20, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=21, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=22, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=23, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=24, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=25, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  3  1 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=26, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=27, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=28, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=29, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  1  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=30, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  1  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=31, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=36, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=41, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=46, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  3  1 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=51, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  1  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=56, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=61, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=66, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=71, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=76, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "jumlah fitur:  8\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=10, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 11  1  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  5  0 16]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=11, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 11  1  0]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  5  0 16]]\n",
      "akurasi uji:  0.803921568627451\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=12, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  0  0  1  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  4  1 16]]\n",
      "akurasi uji:  0.803921568627451\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=13, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  5  0 16]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=14, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  1  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  2  1 18]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=15, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  5  0 16]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=16, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  3  0 18]]\n",
      "akurasi uji:  0.8823529411764706\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=17, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 11  1  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  3  0 18]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=18, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  5  0 16]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=19, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=20, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  3  0 18]]\n",
      "akurasi uji:  0.8823529411764706\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=21, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  3  0 18]]\n",
      "akurasi uji:  0.8823529411764706\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=22, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  3  0 18]]\n",
      "akurasi uji:  0.8823529411764706\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=23, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=24, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  1  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  3  0 18]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=25, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  0  1  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=26, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  1  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  3  0 18]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=27, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=28, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  0  0  1  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=29, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=30, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=31, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=36, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  1  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=41, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  3  0 18]]\n",
      "akurasi uji:  0.8823529411764706\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=46, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=51, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=56, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  3  0 18]]\n",
      "akurasi uji:  0.8823529411764706\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=61, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=66, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=71, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  3  0 18]]\n",
      "akurasi uji:  0.8823529411764706\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=76, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  3  0 18]]\n",
      "akurasi uji:  0.8823529411764706\n",
      "\n",
      "jumlah fitur:  9\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=10, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  1  0  3  0]\n",
      " [ 0  0  4  1 16]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=11, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 10  0  2]\n",
      " [ 0  0  0  5  0]\n",
      " [ 0  0  2  0 19]]\n",
      "akurasi uji:  0.8823529411764706\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=12, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  0  0  5  0]\n",
      " [ 0  0  5  0 16]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=13, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  1 16]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=14, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  1  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  5  0 16]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=15, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  1  0  4  0]\n",
      " [ 0  0  4  1 16]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=16, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  0  0  5  0]\n",
      " [ 0  0  5  0 16]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=17, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 11  0  1]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  1 16]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=18, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 11  0  1]\n",
      " [ 0  0  0  5  0]\n",
      " [ 0  0  3  1 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=19, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 11  0  1]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  5  0 16]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=20, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  5  0 16]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=21, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  0  1  4  0]\n",
      " [ 0  0  4  0 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=22, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  1 16]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=23, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  1 16]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=24, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  1 16]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=25, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  1  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  2  1 18]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=26, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  1  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  0  0  5  0]\n",
      " [ 0  0  2  0 19]]\n",
      "akurasi uji:  0.9019607843137255\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=27, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 11  0  1]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  4  1 16]]\n",
      "akurasi uji:  0.8235294117647058\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=28, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 11  0  1]\n",
      " [ 0  0  0  5  0]\n",
      " [ 0  0  5  0 16]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=29, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  0  0  5  0]\n",
      " [ 0  0  2  0 19]]\n",
      "akurasi uji:  0.9215686274509803\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=30, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  0  1  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  3  1 17]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=31, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  2  0 19]]\n",
      "akurasi uji:  0.9019607843137255\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=36, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  0  3  1 17]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=41, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  1  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  0  1  4  0]\n",
      " [ 0  0  3  1 17]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=46, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  1  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  0  1  4  0]\n",
      " [ 0  0  2  1 18]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=51, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  1  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  0  1  4  0]\n",
      " [ 0  0  2  1 18]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=56, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  1  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  0  1  4  0]\n",
      " [ 0  0  2  1 18]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=61, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  1  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  0  1  4  0]\n",
      " [ 0  0  3  1 17]]\n",
      "akurasi uji:  0.8431372549019608\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=66, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  1  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  0  1  4  0]\n",
      " [ 0  0  2  1 18]]\n",
      "akurasi uji:  0.8627450980392157\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=71, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[11  0  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  0  1  4  0]\n",
      " [ 0  0  3  0 18]]\n",
      "akurasi uji:  0.8823529411764706\n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=76, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "[[10  1  0  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 0  0 12  0  0]\n",
      " [ 0  0  0  5  0]\n",
      " [ 0  0  3  0 18]]\n",
      "akurasi uji:  0.8823529411764706\n",
      "\n",
      "jumlah fitur:  10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-28500feac051>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# Fungsi fit_transform hanya dilakukan di data latih\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    610\u001b[0m         \"\"\"\n\u001b[0;32m    611\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[1;32m--> 612\u001b[1;33m                         warn_on_dtype=True, estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[1;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    452\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 44\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "data = [ ]\n",
    "k=5\n",
    "#for k in [11,16,21,26,30]:\n",
    "while(k<31):\n",
    "    print(\"jumlah fitur: \",k)\n",
    "    #membuat dataset target y\n",
    "    import numpy as np\n",
    "    y= np.array(dataset)[:,0]\n",
    "\n",
    "    #membuat dataset input X\n",
    "    X = np.array(dataset)[:,1:k]\n",
    "\n",
    "    #keterangan jumlah fitur\n",
    "    data.append([29,k])\n",
    "    \n",
    "    # Pisahkan dataset menjadi data latih dan uji dengan perbandingan 80:20\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "    # Penyekalaan fitur dilakukan pada data latih dan uji.\n",
    "    # Detail kenapa ini penting dijelaskan di sini: http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "\n",
    "    # Fungsi fit_transform hanya dilakukan di data latih\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    # Latih kecerdasan buatan dengan data latih, menggunakan MLP-NN\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    import time\n",
    "\n",
    "    i=10\n",
    "   \n",
    "    while (i<80):\n",
    "      clf = MLPClassifier(activation='tanh',solver='adam', alpha=1e-5, max_iter=3000,\n",
    "                        hidden_layer_sizes=(i), random_state=1)\n",
    "      start = time.time()    \n",
    "      print(clf.fit(X_train, y_train))\n",
    "      end = time.time()\n",
    "      waktuTrain = end - start\n",
    "      if(i>30):\n",
    "        i=i+5\n",
    "      elif(i<=30):  \n",
    "        i=i+1\n",
    "\n",
    "       #prediksi\n",
    "      start = time.time()\n",
    "      result = clf.predict(X_test)\n",
    "      end = time.time()\n",
    "      waktuTest = end - start\n",
    "\n",
    "      #Confusion Matriks Hasil Prediksi dengan Nilai Sebenarnya\n",
    "      from sklearn.metrics import classification_report,confusion_matrix, roc_auc_score\n",
    "      print(confusion_matrix(y_test,result))\n",
    "\n",
    "      # Cek kemampuan kecerdasan buatan dengan data uji \n",
    "      test_accuracy = clf.score(X_test, y_test)\n",
    "      print(\"akurasi uji: \",test_accuracy)\n",
    "\n",
    "      # Cek kemampuan kecerdasan buatan dengan data uji \n",
    "      train_accuracy = clf.score(X_train, y_train)\n",
    "\n",
    "      data.append( [i-1,float('{0:.4f}'.format(test_accuracy))*100, '{0:.4f}'.format(clf.loss_), '{0:.0f}'.format(clf.n_iter_), '{0:.4f}'.format(waktuTrain), '{0:.4f}'.format(waktuTest)]) \n",
    "      print()  \n",
    "    k=k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
